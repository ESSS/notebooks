{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Write Strategy\n",
    "Currently HDF5 is our standard file format to store simulation results and this notebook aims to check if playing with raw data yields better perfomance (in terms of both, speed and file size).\n",
    "\n",
    "The idea is to emulate an ALFASim simulation that generates a large amount of data (profiles and trends). We are saving all pipes variables as profiles and for each pipe the trends are defined as the values at the first position just for easiness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some constants to determine the problem size, frequencies and where to store the data to be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number_of_pipes = 10\n",
    "number_of_cells = 500\n",
    "total_simulation_time = 6 * 60 * 60  # [s], 6 hours of simulation\n",
    "\n",
    "trend_output_frequency = 2  # [s]\n",
    "profile_output_frequency = 5 * 60  # [s]\n",
    "\n",
    "initial_time_step = 1.0e-2  # [s]\n",
    "time_step_increase_factor = 1.1\n",
    "maximum_time_step = 2  # [s]\n",
    "\n",
    "import tempfile\n",
    "import os\n",
    "import json\n",
    "import numpy\n",
    "\n",
    "root_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a list of pipes, each one with some output properies and random values.\n",
    "During the simulation the data for each pipe will change but for our pourposes it can be always the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "\n",
    "output_variables_per_pipe = [\n",
    "    'pressure', \n",
    "    'temperature', \n",
    "    'gas_velocity', \n",
    "    'liquid_velocity'\n",
    "]\n",
    "\n",
    "positions = number_of_pipes * [0]\n",
    "\n",
    "class Pipe:\n",
    "    __slots__ = list(output_variables_per_pipe)\n",
    "\n",
    "\n",
    "pipes = []\n",
    "for i in range(number_of_pipes):\n",
    "    pipes.append(Pipe())\n",
    "    for var in output_variables_per_pipe:\n",
    "        setattr(pipes[-1], var, np.random.rand(number_of_cells))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Time step sizes are not constant during the simulation, so emulate a growing time step size up to the point when it reaches the maximum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_time_step(time_step, time_step_increase_factor, maximum_time_step):\n",
    "    new_time_step = time_step * time_step_increase_factor\n",
    "    if new_time_step > maximum_time_step:\n",
    "        return maximum_time_step\n",
    "    else:\n",
    "        return new_time_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw data from NumPy and metadata\n",
    "Crate a folder to store the simulation results. This would have the project name associated to it but here simplename is enough, just check whether it exists or not and clear the folder content if necessary. A folder inside of the main one is also created to store the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dirs():\n",
    "\n",
    "    project_name = 'my_project_folder'\n",
    "    project_path = os.path.join(root_path, project_name)\n",
    "    meta_path = os.path.join(project_path, 'meta')\n",
    "\n",
    "    if not os.path.exists(project_path):\n",
    "        os.makedirs(project_path)\n",
    "    else:\n",
    "        # clear directory content\n",
    "        # in real life need to ask the user what to do, if delete and rerun simulation or stop\n",
    "        import shutil\n",
    "        for file in os.listdir(project_path):\n",
    "            file_path = os.path.join(project_path, file)\n",
    "\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path): \n",
    "                shutil.rmtree(file_path, ignore_errors=True)\n",
    "    \n",
    "    os.makedirs(meta_path)\n",
    "    \n",
    "    return project_path, meta_path\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some auxiliary methods to actually write the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_profiles(pipes, tmp_path):\n",
    "    from numpy import ndarray\n",
    "    \n",
    "    profile_path = os.path.join(tmp_path, 'profiles')\n",
    "    os.makedirs(profile_path)\n",
    "    \n",
    "    metadata = []\n",
    "    pipe_index = 0\n",
    "    profile_index = 0\n",
    "    \n",
    "    for pipe in pipes:\n",
    "        pipe_name = 'pipe_' + '{:03d}'.format(pipe_index)\n",
    "        \n",
    "        for var in sorted(output_variables_per_pipe):\n",
    "            md = {}\n",
    "            output_file = 'profile_' + '{:03d}'.format(profile_index) + '.binary'\n",
    "            data = getattr(pipe, var)\n",
    "            data.tofile(os.path.join(profile_path, output_file))\n",
    "            #np.savez_compressed(os.path.join(profile_path, output_file), a=data)\n",
    "            \n",
    "            md['property_id'] = var\n",
    "            md['edge'] = pipe_name\n",
    "            md['fn'] = output_file\n",
    "            md['size'] = len(data)\n",
    "            \n",
    "            profile_index += 1\n",
    "            metadata.append(md)\n",
    "            \n",
    "        pipe_index += 1\n",
    "\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_trends(pipes, positions, tmp_path):\n",
    "    from numpy import array\n",
    "    \n",
    "    trends_path = os.path.join(tmp_path, 'trends')\n",
    "    os.makedirs(trends_path)\n",
    "    \n",
    "    metadata = []\n",
    "    trend_data = []\n",
    "    i = 0\n",
    "    \n",
    "    output_file = 'trends.binary'\n",
    "    \n",
    "    for pipe, position in zip(pipes, positions):\n",
    "        pipe_name = 'pipe_' + '{:03d}'.format(i)\n",
    "        \n",
    "        for var in sorted(output_variables_per_pipe):\n",
    "            md = {}\n",
    "            \n",
    "            full_data = getattr(pipe, var)\n",
    "            trend_data.append(full_data[position])\n",
    "            \n",
    "            md['property_id'] = var\n",
    "            md['edge'] = pipe_name\n",
    "            md['fn'] = output_file\n",
    "            md['position'] = position\n",
    "            \n",
    "            metadata.append(md)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    trend_data = array(trend_data)\n",
    "    trend_data.tofile(os.path.join(trends_path, output_file))\n",
    "\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_metadata(folder_path, metadata, filename):\n",
    "\n",
    "    with open(os.path.join(folder_path, filename), 'w') as file:\n",
    "        json.dump(obj=metadata, fp=file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_timeset(folder_path, timeset):\n",
    "    ts = numpy.array(timeset)\n",
    "    ts_values_file = os.path.join(folder_path, 'timeset.values.binary')\n",
    "    ts.tofile(ts_values_file)\n",
    "    \n",
    "    content = dict({'unit': 's', 'count': len(timeset), 'fn': 'timeset.values.npy'})\n",
    "    with open(os.path.join(folder_path, 'timeset.json'), 'w') as file:\n",
    "        json.dump(obj=content, fp=file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_metadata_and_timeset(metadata_path, folder_name, profile_metadata, trend_metadata, timeset):\n",
    "    tmp_path = tempfile.mkdtemp(dir=metadata_path)\n",
    "    write_metadata(tmp_path, profile_metadata, 'profiles.json')\n",
    "    write_metadata(tmp_path, trend_metadata, 'trends.json')\n",
    "    write_timeset(tmp_path, timeset)\n",
    "    os.rename(tmp_path, os.path.join(metadata_path, folder_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_timeset_file(timeset, meta_path):\n",
    "\n",
    "    ts = numpy.array(timeset)\n",
    "    tmp_values_file = os.path.join(meta_path, 'tmp.values.binary')\n",
    "    ts.tofile(tmp_values_file)\n",
    "    \n",
    "    tmp_meta_file = os.path.join(meta_path, 'tmp.json')\n",
    "    content = dict({'unit': 's', 'count': len(timeset), 'fn': 'timeset.values.binary'})\n",
    "    with open(tmp_meta_file, 'w') as file:\n",
    "        json.dump(obj=content, fp=file, indent=4)        \n",
    "    \n",
    "    ts_values_file = os.path.join(meta_path, 'timeset.values.binary')\n",
    "    ts_meta_file = os.path.join(meta_path, 'timeset.json')\n",
    "    \n",
    "    if os.path.exists(ts_values_file):\n",
    "        os.remove(ts_values_file)\n",
    "    if os.path.exists(ts_meta_file):\n",
    "        os.remove(ts_meta_file)\n",
    " \n",
    "    os.rename(tmp_values_file, ts_values_file)\n",
    "    os.rename(tmp_meta_file, ts_meta_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the actual simulation loop encapsulated into a function just to facilitate timming it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def solve():    \n",
    "    \n",
    "    time_step_index = 0\n",
    "    t = 0\n",
    "    time_step = initial_time_step\n",
    "\n",
    "    elapsed_time_since_last_trend_output = 0.0\n",
    "    elapsed_time_since_last_profile_output = 0.0\n",
    "\n",
    "    timeset = []\n",
    "\n",
    "    # create basic directory structire if it doesn't exist, if it exits clear content\n",
    "    project_path, meta_path = create_dirs()\n",
    "    meta_folder_name = ''\n",
    "\n",
    "    last_profile_metadata = profile_metadata = None\n",
    "    last_trend_metadata = trend_metadata = None\n",
    "\n",
    "    while t < total_simulation_time:\n",
    "\n",
    "        ts_folder_name = 'TS_' + '{:08d}'.format(time_step_index)\n",
    "        \n",
    "        timeset.append(t)\n",
    "\n",
    "        if time_step_index == 0:\n",
    "            # always save initial condition, profiles and trends\n",
    "            tmp_path = tempfile.mkdtemp(dir=project_path)\n",
    "\n",
    "            profile_metadata = write_profiles(pipes, tmp_path)\n",
    "            trend_metadata = write_trends(pipes, positions, tmp_path)\n",
    "            os.rename(tmp_path, os.path.join(project_path, ts_folder_name))\n",
    "            \n",
    "            # also create the metadata folder if everyting goes fine until here\n",
    "            meta_folder_name = 'meta_' + '{:08d}'.format(time_step_index)\n",
    "            write_metadata_and_timeset(meta_path, meta_folder_name, profile_metadata, trend_metadata, timeset)\n",
    "            \n",
    "            last_profile_metadata = profile_metadata\n",
    "            last_trend_metadata = trend_metadata\n",
    "            \n",
    "        # call solve methods or in this case just use the values in the pipes list\n",
    "\n",
    "        elapsed_time_since_last_trend_output += time_step\n",
    "        elapsed_time_since_last_profile_output += time_step\n",
    "\n",
    "        if elapsed_time_since_last_trend_output > trend_output_frequency or \\\n",
    "            elapsed_time_since_last_profile_output > profile_output_frequency:\n",
    "\n",
    "            tmp_path = tempfile.mkdtemp(dir=project_path)\n",
    "\n",
    "            if elapsed_time_since_last_trend_output > trend_output_frequency:\n",
    "                trend_metadata = write_trends(pipes, positions, tmp_path)\n",
    "                elapsed_time_since_last_trend_output = 0.0\n",
    "\n",
    "            if elapsed_time_since_last_profile_output > profile_output_frequency:\n",
    "                profile_metadata = write_profiles(pipes, tmp_path)\n",
    "                elapsed_time_since_last_profile_output = 0.0\n",
    "\n",
    "            os.rename(tmp_path, os.path.join(project_path, ts_folder_name))\n",
    "            \n",
    "            # update timeset file \n",
    "            update_timeset_file(timeset, os.path.join(meta_path, meta_folder_name))\n",
    "            \n",
    "            # if metadata changed, create a new directory and store the updated information\n",
    "            if last_profile_metadata != profile_metadata or last_trend_metadata != trend_metadata:\n",
    "                meta_folder_name = 'meta_' + '{:08d}'.format(time_step_index)\n",
    "                write_metadata_and_timeset(meta_path, meta_folder_name, profile_metadata, trend_metadata, timeset)\n",
    "\n",
    "                \n",
    "        time_step = new_time_step(time_step, time_step_increase_factor, maximum_time_step)\n",
    "        t += time_step\n",
    "        time_step_index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.3 s ± 420 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "solve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__First implementation was using a `.json` file to store the timeset, but as it can become quite big (all simulated time steps, not only the saved ones) the dump to the `.json` file was a bottleneck: `93.7%` of the time was spent just in `update_timeset_file` method:__\n",
    "\n",
    "```\n",
    "%lprun -f solve solve()\n",
    "\n",
    "Timer unit: 3.01859e-07 s\n",
    "\n",
    "Total time: 378.498 s\n",
    "File: <ipython-input-38-26d1dd4204e0>\n",
    "Function: solve at line 1\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     1                                           def solve():    \n",
    "     2                                               \n",
    "     3         1            6      6.0      0.0      time_step_index = 0\n",
    "     4         1            3      3.0      0.0      t = 0\n",
    "     5         1            3      3.0      0.0      time_step = initial_time_step\n",
    "     6                                           \n",
    "     7         1            2      2.0      0.0      elapsed_time_since_last_trend_output = 0.0\n",
    "     8         1            2      2.0      0.0      elapsed_time_since_last_profile_output = 0.0\n",
    "     9                                           \n",
    "    10         1            3      3.0      0.0      timeset = []\n",
    "    11                                           \n",
    "                                                                                                                                      \n",
    "    12         1     29782315 29782315.0      2.4      project_path, meta_path = create_dirs()\n",
    "    13                                           \n",
    "    14         1           10     10.0      0.0      last_profile_metadata = None\n",
    "    15         1            5      5.0      0.0      last_trend_metadata = None\n",
    "    16                                           \n",
    "    17     10846        38824      3.6      0.0      while t < total_simulation_time:\n",
    "    18                                           \n",
    "                                                                                     \n",
    "                                                 \n",
    "    19     10845       210044     19.4      0.0          ts_folder_name = 'TS_' + '{:08d}'.format(time_step_index)\n",
    "    20                                                   \n",
    "    21     10845        50351      4.6      0.0          timeset.append(t)\n",
    "    22                                           \n",
    "    23     10845        32718      3.0      0.0          if time_step_index == 0:\n",
    "    24                                                       # always save initial condition \n",
    "    25         1         2049   2049.0      0.0              tmp_path = tempfile.mkdtemp(dir=project_path)\n",
    "    26                                           \n",
    "    27         1       107427 107427.0      0.0              profile_metadata = write_profiles(pipes, tmp_path)\n",
    "    28         1         4265   4265.0      0.0              trend_metadata = write_trends(pipes, positions, tmp_path)\n",
    "    29         1         1919   1919.0      0.0              os.rename(tmp_path, os.path.join(project_path, ts_folder_name))\n",
    "    30                                                       \n",
    "    31                                                       # also create the metadata folder if everyting goes fine until here\n",
    "    32         1           26     26.0      0.0              meta_folder_name = 'meta_' + '{:08d}'.format(time_step_index)\n",
    "    33         1        42555  42555.0      0.0              write_metadata_and_timeset(meta_path, meta_folder_name, profile_metadata, trend_metadata, timeset)\n",
    "    34                                                       \n",
    "    35         1            7      7.0      0.0              last_profile_metadata = profile_metadata\n",
    "    36         1            2      2.0      0.0              last_trend_metadata = trend_metadata\n",
    "    37                                                       \n",
    "    38                                                   # set up and run the model but here this is not necessary as there is no simulation,\n",
    "    39                                                   # we just use the values in the pipes list\n",
    "    40                                           \n",
    "    41     10845        36874      3.4      0.0          elapsed_time_since_last_trend_output += time_step\n",
    "    42     10845        33156      3.1      0.0          elapsed_time_since_last_profile_output += time_step\n",
    "    43                                           \n",
    "    44     10845        42624      3.9      0.0          if elapsed_time_since_last_trend_output > trend_output_frequency or             elapsed_time_since_last_profile_output > profile_output_frequency:\n",
    "    45                                           \n",
    "    46      5437      5030271    925.2      0.4              tmp_path = tempfile.mkdtemp(dir=project_path)\n",
    "    47                                           \n",
    "    48      5437        57617     10.6      0.0              if elapsed_time_since_last_trend_output > trend_output_frequency:\n",
    "    49      5402     23472668   4345.2      1.9                  trend_metadata = write_trends(pipes, positions, tmp_path)\n",
    "    50      5402        37765      7.0      0.0                  elapsed_time_since_last_trend_output = 0.0\n",
    "    51                                           \n",
    "    52      5437        34341      6.3      0.0              if elapsed_time_since_last_profile_output > profile_output_frequency:\n",
    "    53        71      7417821 104476.4      0.6                  profile_metadata = write_profiles(pipes, tmp_path)\n",
    "    54        71          685      9.6      0.0                  elapsed_time_since_last_profile_output = 0.0\n",
    "    55                                           \n",
    "    56      5437     11762379   2163.4      0.9              os.rename(tmp_path, os.path.join(project_path, ts_folder_name))\n",
    "    57                                                       \n",
    "    58                                                       # update timeset file \n",
    "    59      5437   1174911177 216095.5     93.7              update_timeset_file(timeset, os.path.join(meta_path, meta_folder_name))\n",
    "    60                                                       \n",
    "    61                                                       # if metadata changed, create a new directory and store the updated information\n",
    "    62      5437       548502    100.9      0.0              if last_profile_metadata != profile_metadata or last_trend_metadata != trend_metadata:\n",
    "    63                                                           meta_folder_name = 'meta_' + '{:08d}'.format(time_step_index)\n",
    "    64                                                           write_metadata_and_timeset(meta_path, meta_folder_name, profile_metadata, trend_metadata, timeset)\n",
    "    65                                           \n",
    "    66                                                           \n",
    "    67     10845       143380     13.2      0.0          time_step = new_time_step(time_step, time_step_increase_factor, maximum_time_step)\n",
    "    68     10845        48121      4.4      0.0          t += time_step\n",
    "    69     10845        39312      3.6      0.0          time_step_index += 1\n",
    "```\n",
    "\n",
    "__In more detail:__\n",
    "```\n",
    "%lprun -f update_timeset_file solve()\n",
    "\n",
    "Timer unit: 3.01859e-07 s\n",
    "\n",
    "Total time: 356.392 s\n",
    "File: <ipython-input-37-7e67500ff9fe>\n",
    "Function: update_timeset_file at line 1\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     1                                           def update_timeset_file(timeset, meta_path):\n",
    "     2                                           \n",
    "                                                                              \n",
    "     3      5437       369593     68.0      0.0      tmp_file = os.path.join(meta_path, 'tmp.json')\n",
    "                                                                             \n",
    "                                                     \n",
    "     4      5437       320227     58.9      0.0      timeset_file = os.path.join(meta_path, 'timeset.json')\n",
    "     5                                           \n",
    "     6      5437        64538     11.9      0.0      content = dict({'unit': 's', 'values': timeset})\n",
    "     7      5437      3746910    689.2      0.3      with open(tmp_file, 'w') as file:\n",
    "     8      5437   1156926499 212787.7     98.0          json.dump(obj=content, fp=file, indent=4)    \n",
    "                                                     \n",
    "                                                                                                               \n",
    "     9                                               \n",
    "                                                     \n",
    "    10      5437      2704180    497.4      0.2      if os.path.exists(timeset_file):\n",
    "                                                                                  \n",
    "                                                                                     \n",
    "    11      5437      6365348   1170.7      0.5          os.remove(timeset_file)\n",
    "    12                                            \n",
    "                                                                                               \n",
    "    13      5437     10157657   1868.2      0.9      os.rename(tmp_file, timeset_file)\n",
    "```\n",
    "\n",
    "__The decision was to split the information and save two files, one containing the timeset metadata `timeset.json` and other with the values using a numpy dump `timeset.values`.__\n",
    "```\n",
    "%lprun -f solve solve()\n",
    "\n",
    "Timer unit: 3.01859e-07 s\n",
    "\n",
    "Total time: 35.2779 s\n",
    "File: <ipython-input-11-9d0838aa97cd>\n",
    "Function: solve at line 1\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     1                                           def solve():    \n",
    "     2                                               \n",
    "     3         1            6      6.0      0.0      time_step_index = 0\n",
    "     4         1            3      3.0      0.0      t = 0\n",
    "     5         1            3      3.0      0.0      time_step = initial_time_step\n",
    "     6                                           \n",
    "     7         1            2      2.0      0.0      elapsed_time_since_last_trend_output = 0.0\n",
    "     8         1            3      3.0      0.0      elapsed_time_since_last_profile_output = 0.0\n",
    "     9                                           \n",
    "    10         1            2      2.0      0.0      timeset = []\n",
    "    11                                           \n",
    "    12                                               # create basic directory structire if it doesn't exist, if it exits clear content\n",
    "    13         1       197836 197836.0      0.2      project_path, meta_path = create_dirs()\n",
    "    14         1            7      7.0      0.0      meta_folder_name = ''\n",
    "    15                                           \n",
    "    16         1            3      3.0      0.0      last_profile_metadata = profile_metadata = None\n",
    "    17         1            2      2.0      0.0      last_trend_metadata = trend_metadata = None\n",
    "                                                                                     \n",
    "    18                                           \n",
    "    19     10846        39773      3.7      0.0      while t < total_simulation_time:\n",
    "    20                                           \n",
    "    21     10845       165287     15.2      0.1          ts_folder_name = 'TS_' + '{:08d}'.format(time_step_index)\n",
    "    22                                                   \n",
    "    23     10845        47141      4.3      0.0          timeset.append(t)\n",
    "    24                                           \n",
    "    25     10845        31701      2.9      0.0          if time_step_index == 0:\n",
    "    26                                                       # always save initial condition, profiles and trends\n",
    "    27         1          953    953.0      0.0              tmp_path = tempfile.mkdtemp(dir=project_path)\n",
    "    28                                           \n",
    "    29         1       103800 103800.0      0.1              profile_metadata = write_profiles(pipes, tmp_path)\n",
    "    30         1         3713   3713.0      0.0              trend_metadata = write_trends(pipes, positions, tmp_path)\n",
    "    31         1         1826   1826.0      0.0              os.rename(tmp_path, os.path.join(project_path, ts_folder_name))\n",
    "    32                                                       \n",
    "    33                                                       # also create the metadata folder if everyting goes fine until here\n",
    "    34         1           18     18.0      0.0              meta_folder_name = 'meta_' + '{:08d}'.format(time_step_index)\n",
    "    35         1        41405  41405.0      0.0              write_metadata_and_timeset(meta_path, meta_folder_name, profile_metadata, trend_metadata, timeset)\n",
    "    36                                                       \n",
    "    37         1            6      6.0      0.0              last_profile_metadata = profile_metadata\n",
    "    38         1            3      3.0      0.0              last_trend_metadata = trend_metadata\n",
    "    39                                                       \n",
    "                                                                                                                                             \n",
    "    40                                                   # call solve methods or in this case just use the values in the pipes list\n",
    "    41                                           \n",
    "    42     10845        37281      3.4      0.0          elapsed_time_since_last_trend_output += time_step\n",
    "    43     10845        32066      3.0      0.0          elapsed_time_since_last_profile_output += time_step\n",
    "    44                                           \n",
    "    45     10845        41421      3.8      0.0          if elapsed_time_since_last_trend_output > trend_output_frequency or             elapsed_time_since_last_profile_output > profile_output_frequency:\n",
    "    46                                           \n",
    "    47      5437      4479454    823.9      3.8              tmp_path = tempfile.mkdtemp(dir=project_path)\n",
    "    48                                           \n",
    "    49      5437        46541      8.6      0.0              if elapsed_time_since_last_trend_output > trend_output_frequency:\n",
    "    50      5402     22644276   4191.8     19.4                  trend_metadata = write_trends(pipes, positions, tmp_path)\n",
    "    51      5402        36690      6.8      0.0                  elapsed_time_since_last_trend_output = 0.0\n",
    "    52                                           \n",
    "    53      5437        32757      6.0      0.0              if elapsed_time_since_last_profile_output > profile_output_frequency:\n",
    "    54        71      7820215 110143.9      6.7                  profile_metadata = write_profiles(pipes, tmp_path)\n",
    "    55        71          714     10.1      0.0                  elapsed_time_since_last_profile_output = 0.0\n",
    "    56                                           \n",
    "    57      5437     12242185   2251.6     10.5              os.rename(tmp_path, os.path.join(project_path, ts_folder_name))\n",
    "    58                                                       \n",
    "    59                                                       # update timeset file \n",
    "    60      5437     68224529  12548.2     58.4              update_timeset_file(timeset, os.path.join(meta_path, meta_folder_name))\n",
    "    61                                                       \n",
    "    62                                                       # if metadata changed, create a new directory and store the updated information\n",
    "    63      5437       392792     72.2      0.3              if last_profile_metadata != profile_metadata or last_trend_metadata != trend_metadata:\n",
    "    64                                                           meta_folder_name = 'meta_' + '{:08d}'.format(time_step_index)\n",
    "    65                                                           write_metadata_and_timeset(meta_path, meta_folder_name, profile_metadata, trend_metadata, timeset)\n",
    "    66                                           \n",
    "    67                                                           \n",
    "    68     10845       122973     11.3      0.1          time_step = new_time_step(time_step, time_step_increase_factor, maximum_time_step)\n",
    "    69     10845        45225      4.2      0.0          t += time_step\n",
    "    70     10845        36260      3.3      0.0          time_step_index += 1\n",
    "\n",
    "```\n",
    "\n",
    "__Again in more detail:__\n",
    "```\n",
    "%lprun -f update_timeset_file solve()    \n",
    "    \n",
    "Timer unit: 3.01859e-07 s\n",
    "\n",
    "Total time: 20.5189 s\n",
    "File: <ipython-input-10-1a69b20160e0>\n",
    "Function: update_timeset_file at line 1\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     1                                           def update_timeset_file(timeset, meta_path):\n",
    "     2                                           \n",
    "     3      5437      6109988   1123.8      9.0      ts = numpy.array(timeset)\n",
    "     4      5437       370292     68.1      0.5      tmp_values_file = os.path.join(meta_path, 'tmp.values')\n",
    "     5      5437     14579577   2681.5     21.4      ts.dump(tmp_values_file)\n",
    "     6                                               \n",
    "     7      5437       455100     83.7      0.7      tmp_meta_file = os.path.join(meta_path, 'tmp.json')\n",
    "                                                 \n",
    "     8      5437        63703     11.7      0.1      content = dict({'unit': 's', 'count': len(timeset), 'fn': 'timeset.values'})\n",
    "     9      5437      3681250    677.1      5.4      with open(tmp_meta_file, 'w') as file:\n",
    "    10      5437      9022392   1659.4     13.3          json.dump(obj=content, fp=file, indent=4)        \n",
    "    11                                               \n",
    "    12      5437       455519     83.8      0.7      ts_values_file = os.path.join(meta_path, 'timeset.values')\n",
    "    13      5437       314816     57.9      0.5      ts_meta_file = os.path.join(meta_path, 'timeset.json')\n",
    "    14                                               \n",
    "    15      5437      2227507    409.7      3.3      if os.path.exists(ts_values_file):\n",
    "    16      5437      5394110    992.1      7.9          os.remove(ts_values_file)\n",
    "    17      5437      2048175    376.7      3.0      if os.path.exists(ts_meta_file):\n",
    "    18      5437      4809399    884.6      7.1          os.remove(ts_meta_file)\n",
    "    19                                            \n",
    "    20      5437      9412867   1731.3     13.8      os.rename(tmp_values_file, ts_values_file)\n",
    "    21      5437      9030373   1660.9     13.3      os.rename(tmp_meta_file, ts_meta_file)    \n",
    "```\n",
    "\n",
    "__Before: 378.498 s__\n",
    "\n",
    "__After: 35.2779 s__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "#%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## HDF5 with Pandas\n",
    "http://glowingpython.blogspot.com.br/2014/08/quick-hdf5-with-pandas.html\n",
    "\n",
    "The inspiration comes from the above link, it seems Pandas' `DataFrame` can be used to easily represent the `HDF` groups, creating and appending data where necessary. Let's try it then..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import HDFStore, DataFrame, set_option\n",
    "\n",
    "set_option('io.hdf.default_format','table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_profiles_2_hdf(pipes, output_file, time_step_index, time):\n",
    "\n",
    "    hdf = HDFStore(output_file, complevel=9, complib='blosc')\n",
    "    \n",
    "    pipe_index = 0\n",
    "    \n",
    "    for pipe in pipes:\n",
    "        pipe_name = 'pipe_' + '{:03d}'.format(pipe_index)\n",
    "        folder_name = 'profiles/time_' + '{:06d}'.format(time_step_index) + '/' + pipe_name\n",
    "        local={}\n",
    "        \n",
    "        for var in output_variables_per_pipe:\n",
    "            data = getattr(pipe, var)\n",
    "            local[var] = data\n",
    "\n",
    "        df = DataFrame(local)\n",
    "        hdf.put(folder_name, df)\n",
    "        pipe_index += 1\n",
    "  \n",
    "    hdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_trends_2_hdf(pipes, positions, output_file, time_step_index, time):\n",
    "\n",
    "    hdf = HDFStore(output_file, complevel=9, complib='blosc')\n",
    "    \n",
    "    pipe_index = 0\n",
    "\n",
    "    for pipe in pipes:\n",
    "        pipe_name = 'pipe_' + '{:03d}'.format(pipe_index)\n",
    "        folder_name = 'trends/' + pipe_name\n",
    "        local={}\n",
    "        \n",
    "        for var, pos in zip(output_variables_per_pipe, positions):\n",
    "            data = getattr(pipe, var)\n",
    "            local[var] = data[pos]\n",
    "\n",
    "        df = DataFrame(local, index=[time_step_index])\n",
    "        if time_step_index == 0:\n",
    "            hdf.put(folder_name, df, index=False)\n",
    "        else:\n",
    "            hdf.append(folder_name, df, index=False)\n",
    "        pipe_index += 1\n",
    "\n",
    "\n",
    "    hdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_timeset_2_hdf(timeset, time_step_index, output_file):\n",
    "    hdf = HDFStore(output_file, complevel=9, complib='blosc')\n",
    "    folder_name = 'timeset'   \n",
    "    \n",
    "    hdf.put(folder_name, DataFrame(timeset))\n",
    "\n",
    "    hdf.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def solve_hdf():    \n",
    "    \n",
    "    time_step_index = 0\n",
    "    t = 0\n",
    "    time_step = initial_time_step\n",
    "\n",
    "    elapsed_time_since_last_trend_output = 0.0\n",
    "    elapsed_time_since_last_profile_output = 0.0\n",
    "\n",
    "    timeset = []\n",
    "\n",
    "    output_file = os.path.join(root_path, 'hdf_results_file.h5')\n",
    "    if os.path.exists(output_file):\n",
    "        os.remove(output_file)\n",
    "\n",
    "    last_profile_metadata = profile_metadata = None\n",
    "    last_trend_metadata = trend_metadata = None\n",
    "\n",
    "    while t < total_simulation_time:\n",
    "      \n",
    "        timeset.append(t)\n",
    "\n",
    "        if time_step_index == 0:\n",
    "            # always save initial condition, profiles and trends\n",
    "            write_profiles_2_hdf(pipes, output_file, time_step_index, t)\n",
    "            write_trends_2_hdf(pipes, positions, output_file, time_step_index, t)\n",
    "            \n",
    "        # call solve methods or in this case just use the values in the pipes list\n",
    "\n",
    "        elapsed_time_since_last_trend_output += time_step\n",
    "        elapsed_time_since_last_profile_output += time_step\n",
    "\n",
    "        if elapsed_time_since_last_trend_output > trend_output_frequency or \\\n",
    "            elapsed_time_since_last_profile_output > profile_output_frequency:\n",
    "\n",
    "            if elapsed_time_since_last_trend_output > trend_output_frequency:\n",
    "                write_trends_2_hdf(pipes, positions, output_file, time_step_index, t)\n",
    "                elapsed_time_since_last_trend_output = 0.0\n",
    "\n",
    "            if elapsed_time_since_last_profile_output > profile_output_frequency:\n",
    "                write_profiles_2_hdf(pipes, output_file, time_step_index, t)\n",
    "                elapsed_time_since_last_profile_output = 0.0\n",
    "            \n",
    "            # update timeset file \n",
    "            update_timeset_2_hdf(timeset, time_step_index, output_file)\n",
    "                \n",
    "        time_step = new_time_step(time_step, time_step_increase_factor, maximum_time_step)\n",
    "        t += time_step\n",
    "        time_step_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%timeit \n",
    "solve_hdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Using compression makes the file size drops from 62,7 Mb to 24,3 Mb.\n",
    "\n",
    "The use of `index=False` while appending data made the total time drops from ~780 to ~400 seconds. \n",
    "\n",
    "Results are almost the same in terms of file size but in terms of time, dumping the raw numpy data showed much faster. The main cause is that appending data to the trends is an expensive operation. \n",
    "If instead of appending trend data, a similar structure as for the profiles is built (save trend data every time), the file gets bigger, around 335 Mb, and the time increases as more elements are added to the hdf file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "lprun -f solve_hdf solve_hdf()\n",
    "\n",
    "Timer unit: 3.01859e-07 s\n",
    "\n",
    "Total time: 561.247 s\n",
    "File: <ipython-input-18-5225533419f9>\n",
    "Function: solve_hdf at line 1\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     1                                           def solve_hdf():    \n",
    "     2                                               \n",
    "     3         1            7      7.0      0.0      time_step_index = 0\n",
    "     4         1            3      3.0      0.0      t = 0\n",
    "     5         1            3      3.0      0.0      time_step = initial_time_step\n",
    "     6                                           \n",
    "     7         1            4      4.0      0.0      elapsed_time_since_last_trend_output = 0.0\n",
    "     8         1            2      2.0      0.0      elapsed_time_since_last_profile_output = 0.0\n",
    "     9                                           \n",
    "    10         1            3      3.0      0.0      timeset = []\n",
    "    11                                           \n",
    "    12         1          140    140.0      0.0      output_file = os.path.join(root_path, 'hdf_results_file.h5')\n",
    "    13         1         2035   2035.0      0.0      if os.path.exists(output_file):\n",
    "    14         1         9709   9709.0      0.0          os.remove(output_file)\n",
    "    15                                           \n",
    "    16         1            9      9.0      0.0      last_profile_metadata = profile_metadata = None\n",
    "    17         1            5      5.0      0.0      last_trend_metadata = trend_metadata = None\n",
    "    18                                           \n",
    "    19     10846        38872      3.6      0.0      while t < total_simulation_time:\n",
    "    20                                                 \n",
    "    21     10845        39093      3.6      0.0          timeset.append(t)\n",
    "    22                                           \n",
    "    23     10845        29691      2.7      0.0          if time_step_index == 0:\n",
    "    24                                                       # always save initial condition, profiles and trends\n",
    "    25         1       615936 615936.0      0.0              write_profiles_2_hdf(pipes, output_file, time_step_index, t)\n",
    "    26         1       250852 250852.0      0.0              write_trends_2_hdf(pipes, positions, output_file, time_step_index, t)\n",
    "    27                                                       \n",
    "    28                                                   # call solve methods or in this case just use the values in the pipes list\n",
    "    29                                           \n",
    "    30     10845        33713      3.1      0.0          elapsed_time_since_last_trend_output += time_step\n",
    "    31     10845        31333      2.9      0.0          elapsed_time_since_last_profile_output += time_step\n",
    "    32                                           \n",
    "    33     10845        42973      4.0      0.0          if elapsed_time_since_last_trend_output > trend_output_frequency or             elapsed_time_since_last_profile_output > profile_output_frequency:\n",
    "    34                                           \n",
    "    35      5437        14523      2.7      0.0              if elapsed_time_since_last_trend_output > trend_output_frequency:\n",
    "    36      5402   1437442723 266094.5     77.3                  write_trends_2_hdf(pipes, positions, output_file, time_step_index, t)\n",
    "    37      5402        44686      8.3      0.0                  elapsed_time_since_last_trend_output = 0.0\n",
    "    38                                           \n",
    "    39      5437        50935      9.4      0.0              if elapsed_time_since_last_profile_output > profile_output_frequency:\n",
    "    40        71     38007387 535315.3      2.0                  write_profiles_2_hdf(pipes, output_file, time_step_index, t)\n",
    "    41        71          593      8.4      0.0                  elapsed_time_since_last_profile_output = 0.0\n",
    "    42                                                       \n",
    "    43                                                       # update timeset file \n",
    "    44      5437    382398143  70332.6     20.6              timeset = update_timeset_2_hdf(timeset, time_step_index, output_file)\n",
    "    45                                                           \n",
    "    46     10845       167670     15.5      0.0          time_step = new_time_step(time_step, time_step_increase_factor, maximum_time_step)\n",
    "    47     10845        44826      4.1      0.0          t += time_step\n",
    "    48     10845        34471      3.2      0.0          time_step_index += 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "lprun -f write_trends_2_hdf solve_hdf()\n",
    "\n",
    "Timer unit: 3.01859e-07 s\n",
    "\n",
    "Total time: 466.429 s\n",
    "File: <ipython-input-16-0960896438b6>\n",
    "Function: write_trends_2_hdf at line 1\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     1                                           def write_trends_2_hdf(pipes, positions, output_file, time_step_index, time):\n",
    "     2                                           \n",
    "     3      5403     26576072   4918.8      1.7      hdf = HDFStore(output_file, complevel=9, complib='blosc')\n",
    "     4                                               \n",
    "     5      5403        33586      6.2      0.0      pipe_index = 0\n",
    "     6                                           \n",
    "     7     59433       254140      4.3      0.0      for pipe in pipes:\n",
    "     8     54030      1539888     28.5      0.1          pipe_name = 'pipe_' + '{:03d}'.format(pipe_index)\n",
    "     9     54030       233666      4.3      0.0          folder_name = 'trends/' + pipe_name\n",
    "    10     54030       317760      5.9      0.0          local={}\n",
    "    11                                                   \n",
    "    12    270150      1168972      4.3      0.1          for var, pos in zip(output_variables_per_pipe, positions):\n",
    "    13    216120      1076875      5.0      0.1              data = getattr(pipe, var)\n",
    "    14    216120      1114198      5.2      0.1              local[var] = data[pos]\n",
    "    15                                           \n",
    "    16     54030    246591786   4564.0     16.0          df = DataFrame(local, index=[time_step_index])\n",
    "    17     54030       319697      5.9      0.0          if time_step_index == 0:\n",
    "    18        10       214424  21442.4      0.0              hdf.put(folder_name, df, index=False)\n",
    "    19                                                   else:\n",
    "    20     54020   1226677630  22707.8     79.4              hdf.append(folder_name, df, index=False)\n",
    "    21     54030       471009      8.7      0.0          pipe_index += 1\n",
    "    22                                           \n",
    "    23                                           \n",
    "    24      5403     38597675   7143.7      2.5      hdf.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There is still the possibiity of using numpy compressed format `numpy.savez_compressed`, in this case:\n",
    "\n",
    "**Before: ** 26.2Mb and 46 s\n",
    "\n",
    "**After: ** 13.9 Mb and 104 s\n",
    "\n",
    "If we try to save the compressed format only for the profiles we can achieve a better total file size (43% reduction) with less impact in time (14% increase):\n",
    "\n",
    "**14.9 Mb and 52.4 s**\n",
    "\n",
    "Since we have now `.npz` and `.npy` (compressed and uncompressed) files, it would be good to have this information in the meta-data files as well as an item even if the same method to load the data can be used `numpy.load`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Despite `numpy.savez_compressed` provides some facility, the file format together with the data introduces some metadata already. It was decided then to use the rawest format as possible, so `ndarray.tofile` was also tested.\n",
    "**11.3Mb and ~40 s**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data\n",
    "In the same way we had some questions about writing data, reading is important also, so let's try retrieving the data from the file(s)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_raw_profile(desired_properties):\n",
    "    project_name = 'my_project_folder'\n",
    "    project_path = os.path.join(root_path, project_name)\n",
    "    meta_path = os.path.join(project_path, 'meta')\n",
    "\n",
    "    if not os.path.exists(project_path):\n",
    "        raise(IOError, 'Can not find the project folder: %s' % project_path)\n",
    "    if not os.path.exists(meta_path):\n",
    "        raise(IOError, 'Can not find the metadata folder: %s' % meta_path)\n",
    "\n",
    "    # in this case there is a single metafile, but we could have more than one\n",
    "    profiles_meta_file = os.path.join(os.path.join(meta_path, 'meta_00000000'), 'profiles.json')\n",
    "    with open(profiles_meta_file) as file:    \n",
    "        profiles_meta = json.load(file)\n",
    "\n",
    "    for desired_property, desired_edge in desired_properties:\n",
    "        meta = None\n",
    "\n",
    "        for pf in profiles_meta:\n",
    "            if pf['edge'] == desired_edge and pf['property_id'] == desired_property:\n",
    "                meta = pf\n",
    "                break\n",
    "\n",
    "        data_fn = meta['fn']\n",
    "\n",
    "        surface = []\n",
    "        for subdir, dirs, files in os.walk(project_path):\n",
    "            for dir in dirs:\n",
    "                if dir.startswith('TS'):\n",
    "                    dir_path = os.path.join(project_path, dir)\n",
    "                    profile_path = os.path.join(dir_path, 'profiles')\n",
    "                    if os.path.exists(profile_path):\n",
    "                        file_path = os.path.join(profile_path, data_fn)\n",
    "                        surface.append(np.fromfile(file_path))\n",
    "    \n",
    "        # now use the array to do whathever you want..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "single_property = [\n",
    "    ('pressure', 'pipe_002')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.83 s ± 12.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# time to read one profile\n",
    "read_raw_profile(single_property)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lprun -f read_raw_profile read_raw_profile(single_property)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_properties = []\n",
    "\n",
    "for i in range(number_of_pipes):\n",
    "    pipe_name = 'pipe_' + '{:03d}'.format(i)\n",
    "    for prop in output_variables_per_pipe:\n",
    "        all_properties.append((prop, pipe_name))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1min 13s ± 664 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# time to read several profiles\n",
    "read_raw_profile(all_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading one profile takes ~2 seconds while reading 40 profiles (4 variables x 10 pipes) takes ~80 seconds, fairly linear relation, which is good..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_hdf_profile(desired_properties):\n",
    "    from pandas import read_hdf\n",
    "    import pandas\n",
    "\n",
    "    with pandas.HDFStore('hdf_results_file.h5') as hdf:\n",
    "        for desired_property, desired_edge in desired_properties:\n",
    "            surface = []\n",
    "            for key in hdf.keys():\n",
    "                if 'profiles' in key and desired_edge in key:\n",
    "                    data = read_hdf('hdf_results_file.h5', key)\n",
    "                    surface.append(data[desired_property].values)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.58 s ± 20.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "read_hdf_profile(single_property)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3min 8s ± 5.36 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "read_hdf_profile(all_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lprun -f read_hdf_profile read_hdf_profile(single_property)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read one property in the HDF takes  ~4.5 seconds and to read 40 properties ~190 seconds (~42 times more), also fairly linear..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
